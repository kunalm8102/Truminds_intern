{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tA2E7UY4YzW"
      },
      "source": [
        "# Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz7ghcWAatK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474f0f5f-d3cd-4225-f2d8-38d9414d189c"
      },
      "source": [
        "# Install requirements\n",
        "!pip install fastapi\n",
        "!pip install opencv-python\n",
        "!pip install Pillow\n",
        "!pip install timm\n",
        "!pip install python-multipart\n",
        "!pip install uvicorn\n",
        "\n",
        "# !pip show timm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.11)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.7.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (1.1.2)\n",
            "Installing collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.100.0 starlette-0.27.0\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (8.4.0)\n",
            "Collecting timm\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 timm-0.9.2\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.6\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.4)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, uvicorn\n",
            "Successfully installed h11-0.14.0 uvicorn-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokMreh-e2YC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0925d061-15e7-47b4-ae59-7e59354e6be7"
      },
      "source": [
        "!pip install nest-asyncio\n",
        "# !pip show timm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.5.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYDZfIQ0ljYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6123768c-dd7d-4d02-f10c-266284d0b171"
      },
      "source": [
        "!pip install pyngrok\n",
        "# !pip show timm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/681.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=87fd056643c40f1214d78b17d0c5b6232743ae3720849d872fcd669085155591\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.responses import HTMLResponse, StreamingResponse\n",
        "\n",
        "import cv2\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from matplotlib.figure import Figure\n",
        "\n",
        "# !pip show timm"
      ],
      "metadata": {
        "id": "EIQ3BGWgNAe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DPT Model"
      ],
      "metadata": {
        "id": "CBkSccNGQhE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_dpt(model_type):\n",
        "  ## Load model\n",
        "\n",
        "  midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  midas.to(device)\n",
        "  midas.eval()\n",
        "\n",
        "  return midas\n",
        "\n",
        "\n",
        "def pre_process_dpt(image, model_type):\n",
        "\n",
        "    midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "    if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "        transform = midas_transforms.dpt_transform\n",
        "    else:\n",
        "        transform = midas_transforms.small_transfor\n",
        "\n",
        "    # Load image\n",
        "    img = cv2.imdecode(np.frombuffer(image.file.read(),\n",
        "                                      np.uint8),\n",
        "                        cv2.IMREAD_COLOR)\n",
        "\n",
        "    # convert it to the correct format\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Transform it so that it can be used by the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    # Return this image so it can be used in postprocessing\n",
        "    return input_batch, img\n",
        "\n",
        "def post_process_dpt(original, prediction):\n",
        "\n",
        "  prediction = torch.nn.functional.interpolate(\n",
        "                prediction.unsqueeze(1),\n",
        "                size=original.shape[:2],\n",
        "                mode=\"bicubic\",\n",
        "                align_corners=False,\n",
        "            ).squeeze()\n",
        "\n",
        "  output = prediction.cpu().numpy()\n",
        "  # Create a figure using matplotlib which super-imposes the original\n",
        "  # image and the prediction\n",
        "\n",
        "  fig = Figure()\n",
        "  canvas = FigureCanvas(fig)\n",
        "  ax = fig.gca()\n",
        "\n",
        "  # Render both images original as foreground\n",
        "  ax.imshow(original)\n",
        "  ax.imshow(output)\n",
        "\n",
        "  ax.axis(\"off\")\n",
        "  canvas.draw()\n",
        "\n",
        "  # Reshape output to be a numpy array\n",
        "  width, height = fig.get_size_inches() * fig.get_dpi()\n",
        "  width = int(width)\n",
        "  height = int(height)\n",
        "  output_image = np.frombuffer(canvas.tostring_rgb(),\n",
        "                                dtype='uint8').reshape(height, width, 3)\n",
        "\n",
        "  # Encode to png\n",
        "  res, im_png = cv2.imencode(\".png\", output_image)\n",
        "\n",
        "  return im_png\n",
        "\n",
        "# !pip show timm\n"
      ],
      "metadata": {
        "id": "rsCQ-SqcLLVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhOgeKe4s3ij",
        "outputId": "1ce9d034-1069-4470-ef26-37beae548d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: timm\n",
            "Version: 0.9.2\n",
            "Summary: PyTorch Image Models\n",
            "Home-page: https://github.com/huggingface/pytorch-image-models\n",
            "Author: Ross Wightman\n",
            "Author-email: ross@huggingface.co\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface-hub, pyyaml, safetensors, torch, torchvision\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd4njq644OGL"
      },
      "source": [
        "# Setup the server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANG-77q5arAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4562743e-d7e0-439e-b35c-329ce29b5450"
      },
      "source": [
        "# !pip install timm==0.6.13\n",
        "model_type = \"DPT_Large\"\n",
        "model = load_model_dpt(model_type)\n",
        "\n",
        "# Code from: https://fastapi.tiangolo.com/tutorial/request-files/\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "@app.post(\"/uploadfiles/\")\n",
        "async def create_upload_files(files: List[UploadFile] = File(...)):\n",
        "    \"\"\" Create API endpoint to send image to and specify\n",
        "     what type of file it'll take\n",
        "\n",
        "    :param files: Get image files, defaults to File(...)\n",
        "    :type files: List[UploadFile], optional\n",
        "    :return: A list of png images\n",
        "    :rtype: list(bytes)\n",
        "    \"\"\"\n",
        "\n",
        "    for image in files:\n",
        "\n",
        "        # Return preprocessed input batch and loaded image\n",
        "        input_batch, image = pre_process_dpt(image, model_type)\n",
        "\n",
        "        # Run the model and postpocess the output\n",
        "        with torch.no_grad():\n",
        "            prediction = model(input_batch)\n",
        "\n",
        "        # # Post process and stitch together the two images to return them\n",
        "        output_image = post_process_dpt(image, prediction)\n",
        "\n",
        "        return StreamingResponse(io.BytesIO(output_image.tobytes()),\n",
        "                                 media_type=\"image/png\")\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def main():\n",
        "    \"\"\"Create a basic home page to upload a file\n",
        "\n",
        "    :return: HTML for homepage\n",
        "    :rtype: HTMLResponse\n",
        "    \"\"\"\n",
        "\n",
        "    content = \"\"\"<body>\n",
        "          <h3>Upload an image to get it's depth map from the MiDaS model</h3>\n",
        "          <form action=\"/uploadfiles/\" enctype=\"multipart/form-data\" method=\"post\">\n",
        "              <input name=\"files\" type=\"file\" multiple>\n",
        "              <input type=\"submit\">\n",
        "          </form>\n",
        "      </body>\n",
        "      \"\"\"\n",
        "    return HTMLResponse(content=content)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/intel-isl/MiDaS/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "Downloading: \"https://github.com/isl-org/MiDaS/releases/download/v3/dpt_large_384.pt\" to /root/.cache/torch/hub/checkpoints/dpt_large_384.pt\n",
            "100%|██████████| 1.28G/1.28G [01:19<00:00, 17.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2SVOcAUAvBqGEo50Mc035SEdyIU_DMHingLrdYSifWjb5FWy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZTOu8ZKvG4X",
        "outputId": "26004a83-d133-4acd-f41b-af3435bdbed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2SVOcAUAvBqGEo50Mc035SEdyIU_DMHingLrdYSifWjb5FWy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2b7FTHW4VEf",
        "outputId": "db19df5c-d1ac-4f81-de66-b4a8a92bbdca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "auth_token = \"2SVOcAUAvBqGEo50Mc035SEdyIU_DMHingLrdYSifWjb5FWy\"\n",
        "\n",
        "# Run the ngrok authentication command\n",
        "command = f\"ngrok authtoken {auth_token}\"\n",
        "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "output, error = process.communicate()\n",
        "\n",
        "# Check if any error occurred during authentication\n",
        "if error:\n",
        "    print(\"Error occurred during authentication:\")\n",
        "    print(error.decode(\"utf-8\"))\n",
        "else:\n",
        "    print(\"Ngrok authentication successful!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kau1g6n9-i2i",
        "outputId": "8f1bfadf-7f64-4d18-b6c0-c652d517cc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok authentication successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Create tunnel\n",
        "public_url = ngrok.connect(8000, bind_tls=True, proto=\"http\")\n",
        "\n",
        "# Print the public URL\n",
        "print(\"Public URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTv6RfAX_GP9",
        "outputId": "6d01bdc2-5e9a-46d3-dec7-b788a3e70784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-07-14T09:03:21+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://1776-34-143-212-154.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hqiEAo5PpLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df2f8bb-edaf-4e95-f840-48445a359239"
      },
      "source": [
        "# Check if it exists\n",
        "!ps aux | grep ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        1430 19.0  0.2 736704 27764 ?        Sl   09:03   0:00 /usr/local/lib/python3.10/dist-packages/pyngrok/bin/ngrok start --none --log=stdout\n",
            "root        1444  0.0  0.0   6904  3188 ?        S    09:03   0:00 /bin/bash -c ps aux | grep ngrok\n",
            "root        1446  0.0  0.0   6444   652 ?        S    09:03   0:00 grep ngrok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NVUy4iL43jg"
      },
      "source": [
        "# Make magic happen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggazuIY0auLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e1bed3-d9c0-4e46-8ab6-a0310e665f83"
      },
      "source": [
        "import nest_asyncio\n",
        "\n",
        "# Allow for asyncio to work within the Jupyter notebook cell\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import uvicorn\n",
        "\n",
        "# Run the FastAPI app using uvicorn\n",
        "print(public_url)\n",
        "uvicorn.run(app)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [543]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"https://1776-34-143-212-154.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "INFO:     2402:e280:21c4:129:a4c5:2bb4:63d4:63ff:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2402:e280:21c4:129:a4c5:2bb4:63d4:63ff:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:e280:21c4:129:a4c5:2bb4:63d4:63ff:0 - \"GET / HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2402:e280:21c4:129:a4c5:2bb4:63d4:63ff:0 - \"POST /uploadfiles/ HTTP/1.1\" 200 OK\n",
            "INFO:     2402:e280:21c4:129:a4c5:2bb4:63d4:63ff:0 - \"GET /uploadfiles/ HTTP/1.1\" 405 Method Not Allowed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [543]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAgM7AQHgh8N"
      },
      "source": [
        "# Kill tunnel\n",
        "ngrok.disconnect(public_url=public_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "30W96DhCN05k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}